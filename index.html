<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-G2956GM9YZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-G2956GM9YZ');
  </script>
<link rel="icon" href="./Mohammad_files/icon.png">
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109957767-1');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">

    @font-face {font-family: "FuturaBookC";
        src: url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.eot"); /* IE9*/
        src: url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.eot?#iefix") format("embedded-opentype"), /* IE6-IE8 */
        url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.woff2") format("woff2"), /* chrome firefox */
        url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.woff") format("woff"), /* chrome firefox */
        url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.ttf") format("truetype"), /* chrome firefox opera Safari, Android, iOS 4.2+*/
        url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.svg#FuturaBookC") format("svg"); /* iOS 4.1- */
    }

    .names {
      font-size: 14.3px;
    }
    .abstract {
      font-size: 16.3px;
    }
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: "FuturaBookC";
    font-size: 20px
    }
    strong {
    font-family: "FuturaBookC";
    font-size: 20px;
    }
    heading {
    font-family: "FuturaBookC";
    font-size: 25px;
    }
    papertitle {
    font-family: "FuturaBookC";
    font-size: 20px;
    font-weight: 700
    }
    name {
    font-family: "FuturaBookC";
    font-size: 38px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <title>Mohammad Pezeshki</title>

  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Mohammad Pezeshki</name>
        </p>
        <p align="justify">I am a PhD student at <a href="https://mila.quebec/en/">Mila</a>, Université de Montréal. My research is supervised by <a href="https://www.guillaumelajoie.com/">Guillaume Lajoie</a> and <a href="https://mila.quebec/en/yoshua-bengio/">Yoshua Bengio</a>. I also closely work with <a href="https://mila.quebec/en/person/aaron-courville/">Aaron Courville</a> and <a href="https://www.cs.mcgill.ca/~dprecup/">Doina Precup</a>. I have received my Masters in Computer Science and my Bachelors in Computer Engineering from <a href="https://www.umontreal.ca/en/">Université de Montréal</a> and <a href="https://aut.ac.ir/en">Tehran Polytechnic</a>, respectively.
        </p>

        <p align="center">
          <a href="mailto:mohammad.pezeshki@umontreal.ca">Email</a> &nbsp;/&nbsp;
          <a href="https://github.com/mpezeshki">GitHub</a> &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?user=HT85tXsAAAAJ&hl=en"> Scholar </a>
        </p>
        </td>
        <td width="33%">
        <img src="./Mohammad_files/photo.png"  alt="prl" width="250" height="246">
        </td>
      </tr>
      </tbody></table>

  
        <heading>Research</heading>

      <p  align="justify">
                I am interested in theories of learning from a dynamical system perspective. I actively follow researches on representation learning, optimization, and generalization. My current focus is on understanding the generalization behaviors of gradient descent.
      </p>

      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Mohammad_files/dd.png" alt="prl" width="250" height="250"></td>
        <td width="75%" valign="top">
        <p>
          <a href="./DD.html">
          <papertitle>Multi-scale Feature Learning Dynamics: Insights for Double Descent</papertitle></a><br>
        </p><p class="names">
          Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, Guillaume Lajoie
        </p><p align="justify" class="abstract"><br>
          We investigate the origins of the epoch-wise double descent, a phenomenon in which the test error undergoes two descents as the training time increases. By leveraging tools from statistical physics, we study a linear teacher-student setup exhibiting epoch-wise double descent. We derive closed-form analytical expressions for the evolution of generalization error over training. We find that double descent can be attributed to distinct features being learned at different scales.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->

      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Mohammad_files/gs.jpg" alt="prl" width="250" height="250"></td>
        <td width="75%" valign="top">
        <p>
          <a href="./GradientStarvation.html">
          <papertitle>Gradient Starvation: A Learning Proclivity in Neural Networks</papertitle></a><br>
        </p><p class="names">
          Mohammad Pezeshki, Sékou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, Guillaume Lajoie
        </p><p align="justify" class="abstract"><br>
          We identify and formalize a fundamental gradient descent phenomenon resulting in a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for classification, despite the presence of other predictive features which fail to be discovered. Please check out our paper and its blog post.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Mohammad_files/ld.jpg" alt="prl" width="250" height="185"></td>
        <td width="75%" valign="top">
        <p>
          <a href="https://arxiv.org/abs/1809.06848">
          <papertitle>On the Learning Dynamics of Deep Neural Networks</papertitle></a><br>
        </p><p class="names">
          Remi Tachet des Combes, Mohammad Pezeshki, Samira Shabanian, Aaron Courville, Yoshua Bengio
        </p><p align="justify" class="abstract"><br>
          In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Mohammad_files/lad.jpg" alt="prl" width="250" height="190"></td>
        <td width="75%" valign="top">
        <p>
          <a href="http://proceedings.mlr.press/v48/pezeshki16.pdf">
          <papertitle>Deconstructing the Ladder Network Architecture</papertitle></a><br>
        </p><p class="names">
        Mohammad Pezeshki, Linxi Fan, Philemon Brakel, Aaron Courville, Yoshua Bengio 
        </p><p align="justify" class="abstract"><br>
          This paper presents an extensive experimental investigation of variants of the Ladder Network in which we replaced or removed individual components to learn about their relative importance. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connections, followed by the application of noise.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->



      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Mohammad_files/nm.jpg" alt="prl" width="250" height="250"></td>
        <td width="75%" valign="top">
        <p>
          <a href="http://proceedings.mlr.press/v89/gidel19a/gidel19a.pdf">
          <papertitle>Negative momentum for improved game dynamics</papertitle></a><br>
        </p><p class="names">
          Gauthier Gidel<sup>*</sup>, Reyhane Askari Hemmat<sup>*</sup>, Mohammad Pezeshki, Rémi Le Priol, Gabriel Huang, Simon Lacoste-Julien, Ioannis Mitliagkas
        </p><p align="justify" class="abstract"><br>
          In this paper, we analyze gradient-based methods with momentum on games. We prove that alternating updates are more stable than simultaneous updates. We show both that alternating gradient updates with a negative momentum term achieves convergence in a difficult toy adversarial problem, but also on the notoriously difficult to train saturating GANs.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Mohammad_files/zo.jpg" alt="prl" width="250" height="220"></td>
        <td width="75%" valign="top">
        <p>
          <a href="https://arxiv.org/abs/1606.01305">
          <papertitle>Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations</papertitle></a><br>
        </p><p class="names">
          David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, Christopher Pal
        </p><p align="justify" class="abstract"><br>
          We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          <a href="https://jonbarron.info/">Thanks Jon!</a>
	    </font>
        </p>
        </td>
      </tr>
      </tbody></table>
    </td>
    </tr>
  </tbody></table>
  

</body></html>
